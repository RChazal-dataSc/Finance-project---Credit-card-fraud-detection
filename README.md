ğŸ’³ Credit Card Fraud Detection

ğŸ¯ Objectif

DÃ©tecter automatiquement les transactions bancaires frauduleuses parmi un grand volume de transactions.
Dataset utilisÃ© : Kaggle â€“ Credit Card Fraud Detection.

Transactions totales : 284 807

Fraudes : 492 (0.17%)

ParticularitÃ© : classes extrÃªmement dÃ©sÃ©quilibrÃ©es

ğŸ”§ Pipeline
1. Exploration et prÃ©paration des donnÃ©es

VÃ©rification des valeurs manquantes et infinies â†’ aucune.
Analyse de la distribution des variables et de la corrÃ©lation avec la cible.
Normalisation des donnÃ©es avec StandardScaler.
Suppression de la variable Time (faible intÃ©rÃªt).

2. Gestion du dÃ©sÃ©quilibre

Application de Borderline-SMOTE pour rÃ©Ã©quilibrer les classes (492 â†’ 284k Ã©chantillons synthÃ©tiques).

3. ModÃ©lisation

Trois approches principales ont Ã©tÃ© testÃ©es :

Isolation Forest (non supervisÃ©, adaptÃ© aux anomalies rares) â†’ rÃ©sultats insatisfaisants.

ModÃ¨les supervisÃ©s classiques :

RÃ©gression Logistique

Random Forest

RÃ©seau de Neurones (ANN) :
Input layer (64 neurones, ReLU)
Hidden layers (32 & 16 neurones, ReLU + Dropout)
Output layer (sigmoÃ¯de)
Optimiseur Adam

ğŸ“Š RÃ©sultats
ModÃ¨le	Accuracy	Precision	Recall	F1-score
Isolation Forest	âŒ	-	-	-
RÃ©gression Logistique	0.986	0.985	0.986	0.986
Random Forest	0.9998	0.9998	0.9998	0.9998
ANN (Keras)	0.9988	0.9984	0.9991	0.9988

ğŸ‘‰ Meilleures performances : Random Forest & ANN

La Random Forest reste la plus simple et robuste.
Lâ€™ANN a produit des rÃ©sultats comparables, tout en montrant un bon Ã©quilibre prÃ©cision/rappel.

ğŸš€ DÃ©ploiement (bonus)

Un modÃ¨le Random Forest a Ã©tÃ© sauvegardÃ© (pickle) et intÃ©grÃ© dans une API Flask minimale :
EntrÃ©e : caractÃ©ristiques dâ€™une transaction
Sortie : prÃ©diction binaire

0 â†’ Transaction normale
1 â†’ Fraude dÃ©tectÃ©e

ğŸ“š BibliothÃ¨ques utilisÃ©es

pandas, numpy, matplotlib, seaborn
scikit-learn, imbalanced-learn
tensorflow.keras
flask
etc...

ğŸ§  Perspectives

AmÃ©lioration du rÃ©seau de neurones (plus dâ€™epochs, tuning des hyperparamÃ¨tres).
Test dâ€™algorithmes de boosting (XGBoost, LightGBM).
DÃ©ploiement dâ€™une API complÃ¨te avec documentation (Swagger / FastAPI).
